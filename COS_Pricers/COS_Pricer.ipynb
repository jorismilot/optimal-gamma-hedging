{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759cadf5",
   "metadata": {},
   "source": [
    "# Pricing Options Using COS / FFT Methods of BS, Merton, Kou, and Heston\n",
    "\n",
    "This workbook demonstrates the highly efficient pricing of European options using COS and FFT methods applied to the Black-Scholes (BS), Merton, Kou, and Heston models. It provides step-by-step guidance on how to calibrate these models, compute option prices across various strikes, and compare them to market-implied volatilities. The process is optimized for handling 0 DTE (zero days to expiry) options using high-frequency data.\n",
    "\n",
    "## Loading 0DTE Option Data\n",
    "\n",
    "The data loading process is handled by the `load_0dte_data` method, as demonstrated in the provided Python code. This method performs the following steps:\n",
    "\n",
    "- Reads raw option data from CSV files using `pandas`.\n",
    "- Processes timestamps and symbols to extract relevant information.\n",
    "\n",
    "Although no explicit data preparation is required, this method sets up the foundation for accessing and analyzing the required market data for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1183e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import scipy.stats as st\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "i = 1j    # imag unit\n",
    "SEC_PER_YEAR = 365 * 24 * 3600 # seconds to years\n",
    "\n",
    "def load_0dte_data(hour='08'):\n",
    "    \"\"\"\n",
    "    Load 0DTE option data for BTC and ETH from CSV files.\n",
    "    The data is filtered to include only options that are expiring today.\n",
    "    Returns a tuple of (btc_df, eth_df).\n",
    "    \"\"\"\n",
    "    btc_df = pd.read_csv(os.path.join(f'/Users/joris/Documents/Master QF/Thesis/optimal-gamma-hedging/Data/calibration_data/{hour}', f'btc_{hour}_0dte_data.csv'))\n",
    "    eth_df = pd.read_csv(os.path.join(f'/Users/joris/Documents/Master QF/Thesis/optimal-gamma-hedging/Data/calibration_data/{hour}', f'eth_{hour}_0dte_data.csv'))\n",
    "\n",
    "    # Convert the timestamps to UTC \n",
    "    btc_df['timestamp'] = pd.to_datetime(btc_df['timestamp'], utc=True)\n",
    "    eth_df['timestamp'] = pd.to_datetime(eth_df['timestamp'], utc=True)\n",
    "    return btc_df, eth_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f0fecb",
   "metadata": {},
   "source": [
    "## Black-Scholes Functions:\n",
    "The code includes functions to calculate Black-Scholes option prices for both call and put options. Additionally, the `implied_volatility` function is used to compute the volatility that corresponds to an observed market option price. It does this using a numerical solver to match the market price to the Black-Scholes formula. This step is critical for model calibration, as it enables the comparison of implied volatilities generated by the model to those observed in the market.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35361be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_inputs_from_df(df):\n",
    "    return (\n",
    "        df['opt_type'].values,\n",
    "        df['spot'].values,\n",
    "        df['strike'].values,        \n",
    "        df['time_to_maturity'].values,\n",
    "        df['mark_price'].values,\n",
    "        df['mark_iv'].values / 100.0  \n",
    "        )\n",
    "\n",
    "# –––––––––––––––– Black-Scholes Prices ––––––––––––––––\n",
    "def bs_price(CP, S0, K, sigma, tau_sec, r):\n",
    "    \"\"\"Vectorised BS call/put price. tau_sec is time‑to‑expiry in seconds.\"\"\"\n",
    "    tau = np.asarray(tau_sec, dtype=float) / SEC_PER_YEAR          # years\n",
    "    CP  = np.asarray(CP, dtype=str)\n",
    "\n",
    "    d1  = (np.log(S0 / K) + (r + 0.5 * sigma**2) * tau) / (sigma * np.sqrt(tau))\n",
    "    d2  = d1 - sigma * np.sqrt(tau)\n",
    "\n",
    "    price = np.where(\n",
    "        CP == 'call',\n",
    "        st.norm.cdf(d1) * S0 - st.norm.cdf(d2) * K * np.exp(-r * tau),\n",
    "        st.norm.cdf(-d2) * K * np.exp(-r * tau) - st.norm.cdf(-d1) * S0\n",
    "    )\n",
    "    return price\n",
    "\n",
    "# –––––––––––––––– Black-Scholes Greeks & IV ––––––––––––––––\n",
    "def bs_delta(CP, S0, K, sigma, tau_sec, r):\n",
    "    tau = tau_sec / SEC_PER_YEAR\n",
    "    d1  = (np.log(S0 / K) + (r + 0.5 * sigma**2) * tau) / (sigma * np.sqrt(tau))\n",
    "    return np.where(CP == 'call', st.norm.cdf(d1), st.norm.cdf(d1) - 1)\n",
    "\n",
    "def bs_gamma(S0, K, sigma, tau_sec, r):\n",
    "    tau = tau_sec / SEC_PER_YEAR\n",
    "    d1  = (np.log(S0 / K) + (r + 0.5 * sigma**2) * tau) / (sigma * np.sqrt(tau))\n",
    "    return st.norm.pdf(d1) / (S0 * sigma * np.sqrt(tau))\n",
    "\n",
    "def bs_vega(S0, K, sigma, tau_sec, r):\n",
    "    tau = tau_sec / SEC_PER_YEAR\n",
    "    d1  = (np.log(S0 / K) + (r + 0.5 * sigma**2) * tau) / (sigma * np.sqrt(tau))\n",
    "    return S0 * st.norm.pdf(d1) * np.sqrt(tau)\n",
    "\n",
    "def bs_impliedvol(CP, marketPrice, K, tau, S0, r):\n",
    "    func = lambda sigma: np.power(bs_price(CP, S0, K, sigma, tau, r) - marketPrice, 1.0)\n",
    "    impliedVol = optimize.newton(func, 0.7, tol=1e-9)\n",
    "    #impliedVol = optimize.brent(func, brack= (0.05, 2))\n",
    "    return impliedVol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ac40f0",
   "metadata": {},
   "source": [
    "## Characteristic Functions Used for Pricing\n",
    "\n",
    "The Python wrappers\n",
    "\n",
    "```python\n",
    "chf_merton(u, ..., S0, K)\n",
    "chf_kou(u, ..., S0, K)\n",
    "chf_heston(u, ..., S0, K)\n",
    "```\n",
    "\n",
    "each return the characteristic function of **log-moneyness**, defined as\n",
    "\n",
    "$$\n",
    "X_\\tau = \\log\\left(\\frac{S_\\tau}{K}\\right), \\qquad\n",
    "x = \\log\\left(\\frac{S_0}{K}\\right), \\qquad\n",
    "\\tau = \\frac{\\texttt{tau\\_sec}}{365 \\times 24 \\times 3600}.\n",
    "$$\n",
    "\n",
    "All wrappers return:\n",
    "\n",
    "$$\n",
    "\\phi_X(u, \\tau) = \\mathbb{E}\\left[e^{iu X_\\tau} \\right] = e^{iux} \\cdot \\varphi_{\\text{model}}(u, \\tau)\n",
    "$$\n",
    "\n",
    "The phase shift $e^{iux}$ is implemented as:\n",
    "\n",
    "```python\n",
    "shift = np.exp(i * u * np.log(S0 / K))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Merton Jump-Diffusion Model\n",
    "\n",
    "```python\n",
    "phi_return = phi_merton(...)\n",
    "phi_X      = shift * phi_return\n",
    "```\n",
    "\n",
    "$$\n",
    "\\phi_X^{\\text{Merton}}(u, \\tau) = e^{iux} \\cdot \\exp\\left[\n",
    "    iu\\mu\\tau - \\frac{1}{2}\\sigma^2 u^2 \\tau + \\xi\\tau \\left(e^{iu\\mu_J - \\frac{1}{2}\\sigma_J^2 u^2} - 1\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\mu = r - \\frac{1}{2}\\sigma^2 - \\underbrace{\\xi\\left(e^{\\mu_J + \\frac{1}{2}\\sigma_J^2} - 1\\right)}_{\\bar{\\omega}}.\n",
    "$$\n",
    "\n",
    "**Interpretation**  \n",
    "Black-Scholes dynamics plus Poisson jumps of normal size $\\mathcal{N}(\\mu_J, \\sigma_J^2)$, arriving at rate $\\xi$.\n",
    "\n",
    "---\n",
    "\n",
    "### Kou Double-Exponential Jump-Diffusion Model\n",
    "\n",
    "```python\n",
    "phi_return = phi_kou(...)\n",
    "phi_X      = shift * phi_return\n",
    "```\n",
    "\n",
    "$$\n",
    "\\phi_X^{\\text{Kou}}(u, \\tau) = e^{iux} \\cdot \\exp\\left[\n",
    "    iu\\mu\\tau - \\frac{1}{2}\\sigma^2 u^2 \\tau + \\xi\\tau \\left( \\frac{p_1\\alpha_1}{\\alpha_1 - iu} + \\frac{p_2\\alpha_2}{\\alpha_2 + iu} - 1 \\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "p_2 = 1 - p_1, \\qquad\n",
    "\\mu = r - \\frac{1}{2}\\sigma^2 - \\xi \\left( \\frac{p_1\\alpha_1}{\\alpha_1 - 1} + \\frac{p_2\\alpha_2}{\\alpha_2 + 1} - 1 \\right)\n",
    "$$\n",
    "\n",
    "**Interpretation**  \n",
    "Jump sizes follow a double-exponential distribution, allowing for asymmetric up/down moves with heavier tails.\n",
    "\n",
    "---\n",
    "\n",
    "### Heston Stochastic Volatility Model\n",
    "\n",
    "```python\n",
    "phi_return = phi_heston(...)\n",
    "phi_X      = shift * phi_return\n",
    "```\n",
    "\n",
    "$$\n",
    "\\phi_X^{\\text{Heston}}(u, \\tau; v_0) = e^{iux} \\cdot \\exp\\left[ A(u, \\tau) + C(u, \\tau) v_0 \\right]\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "\\begin{align}\n",
    "D_1 &= \\sqrt{(\\kappa - i\\rho\\gamma u)^2 + (u^2 + iu)\\gamma^2} \\\\\n",
    "g   &= \\frac{\\kappa - i\\rho\\gamma u - D_1}{\\kappa - i\\rho\\gamma u + D_1} \\\\\n",
    "C(u, \\tau) &= \\frac{1 - e^{-D_1 \\tau}}{\\gamma^2(1 - g e^{-D_1 \\tau})}(\\kappa - i\\rho\\gamma u - D_1) \\\\\n",
    "A(u, \\tau) &= iu r \\tau + \\frac{\\kappa \\bar{v}}{\\gamma^2} \\left[\n",
    "    (\\kappa - i\\rho\\gamma u - D_1)\\tau - 2 \\log\\left( \\frac{1 - g e^{-D_1 \\tau}}{1 - g} \\right)\n",
    "\\right]\n",
    "\\end{align}\n",
    "\n",
    "**Interpretation**  \n",
    "Variance $v_t$ follows a mean-reverting CIR process. Parameters ($\\kappa, \\bar{v}, \\gamma, \\rho$) control long-run variance, vol-of-vol, and spot-vol correlation.\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Note\n",
    "\n",
    "Each wrapper already includes the phase term $e^{iux}$.  \n",
    "So the returned $\\phi_X(u, \\tau)$ can be passed **directly** into a COS or FFT pricer, without modification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40043cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------  MERTON  ------------------\n",
    "def chf_merton(u, r, tau_sec, theta):\n",
    "    sigma, xi, muJ, sigmaJ = theta\n",
    "    tau = (tau_sec / SEC_PER_YEAR)\n",
    "    omega_bar = xi * (np.exp(muJ + 0.5*sigmaJ**2) - 1)\n",
    "    mu = r - 0.5*sigma**2 - omega_bar\n",
    "    return np.exp(i*u*mu*tau -0.5*sigma**2*u**2*tau +\n",
    "                  xi*tau*(np.exp(i*muJ*u - 0.5*sigmaJ**2*u**2) - 1))\n",
    "\n",
    "def phi_merton(S0, K, r, tau_sec, theta):\n",
    "    return lambda u: np.exp(i*u*(np.log(S0) - np.log(K)))  * chf_merton(u, r, tau_sec, theta)\n",
    "\n",
    "# ------------------  KOU  ---------------------\n",
    "def chf_kou(u, r, tau_sec, theta):\n",
    "    sigma, xi, alpha1, alpha2, p1 = theta\n",
    "    p2 = 1 - p1\n",
    "    tau = tau_sec / SEC_PER_YEAR\n",
    "    omega_bar = xi * (p1*alpha1/(alpha1-1) + (1-p1)*alpha2/(alpha2+1) - 1)\n",
    "    mu = r - 0.5*sigma**2 - omega_bar\n",
    "    return np.exp(i*u*mu*tau-0.5*sigma**2*u**2*tau + xi*tau*((p1*alpha1)/(alpha1 - i*u) + (p2*alpha2)/(alpha2 + i*u) - 1))\n",
    "\n",
    "def phi_kou(S0, K, r, tau_sec, theta):\n",
    "    return lambda u: np.exp(i*u*(np.log(S0) - np.log(K))) * chf_kou(u, r, tau_sec, theta)\n",
    "\n",
    "# ------------------  HESTON  ------------------\n",
    "def chf_heston(u, tau_sec, r, theta):\n",
    "    rho, v0, v_bar, kappa, gamma = theta\n",
    "    tau = tau_sec / SEC_PER_YEAR \n",
    "    d1 = np.sqrt((kappa - i*rho*gamma*u)**2 + (u**2 + i*u)*gamma**2)\n",
    "    g  = (kappa - i*rho*gamma*u - d1) / (kappa - i*rho*gamma*u + d1)\n",
    "    term_r  = i*u*r*tau\n",
    "    term_v0 = (v0 / gamma**2) * ((1 - np.exp(-d1*tau)) /\n",
    "                                 (1 - g*np.exp(-d1*tau))) * (kappa - i*rho*gamma*u - d1)\n",
    "    term_bar= (kappa*v_bar / gamma**2) * (tau*(kappa - i*rho*gamma*u - d1) -\n",
    "                                          2*np.log((1 - g*np.exp(-d1*tau))/(1 - g)))\n",
    "    return np.exp(term_r + term_v0 + term_bar)\n",
    "\n",
    "def phi_heston(S0, K, r, tau_sec, theta):\n",
    "    return lambda u: np.exp(i*u*(np.log(S0) - np.log(K))) * chf_heston(u, tau_sec, r, theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882edfa7",
   "metadata": {},
   "source": [
    "## COS Valuation Method (`cos_valuation`):\n",
    "The `cos_valuation` method is the core pricing engine in the code. It uses the COS method, a numerical technique that approximates the option price by expanding the characteristic function into a cosine series. This method is efficient and accurate, especially for pricing European options across a range of strikes. The method takes the characteristic function from one of the models, the spot price, strike price, risk-free rate, time to maturity, and the option type (call or put) as inputs. The integration range `[a, b]` is determined based on the cumulants of the log-return distribution, ensuring accuracy in pricing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62433c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------  Cumulants  ------------------\n",
    "def merton_cumulants(tau, r, theta):\n",
    "    sigma, xi, muJ, sigmaJ = theta\n",
    "    \"\"\" Calculate the cumulants for the Merton model. \"\"\"\n",
    "    omega_bar = xi * (np.exp(muJ + 0.5 * sigmaJ**2) - 1)\n",
    "    c1 = tau * (r - omega_bar - 0.5 * sigma**2 + xi * muJ)\n",
    "    c2 = tau * (sigma**2 + xi * (muJ**2 + sigmaJ**2))\n",
    "    # c4 = tau * xi * (muJ**4 + 6 * muJ**2 * sigmaJ**2 + 3 * sigmaJ**4 * xi)\n",
    "    c4 = tau * xi * (muJ**4 + 6 * muJ**2 * sigmaJ**2 + 3 * sigmaJ**4)\n",
    "    return c1, c2, c4\n",
    "\n",
    "def kou_cumulants(tau, r, theta):\n",
    "    \"\"\" Calculate the cumulants for the Heston model. \"\"\"\n",
    "    sigma, xi, alpha1, alpha2, p1 = theta\n",
    "    p2 = 1 - p1\n",
    "    omega_bar = xi * ((p1*alpha1)/(alpha1-1) + (p2*alpha2)/(alpha2+1) - 1)\n",
    "    c1 = tau * (r - omega_bar - 0.5 * sigma**2 + ((xi * p1)/alpha1 - (xi * p2)/alpha2))\n",
    "    c2 = tau * (sigma**2 + 2 * (xi * p1)/ alpha1**2 + 2 * (xi * p2)/alpha2**2)\n",
    "    c4 = 24 * tau * xi * (p1 / alpha1**4 + p2 / alpha2**4) \n",
    "    return c1, c2, c4\n",
    "\n",
    "# ------------------  COS TRUNCATION WINDOW  ------------------\n",
    "def truncation_window(model, S0, K, tau, r, theta, L=8):\n",
    "    \"\"\" Compute COS truncation window [a, b] for different models. \"\"\"\n",
    "    log_ratio = np.log(S0 / K)\n",
    "\n",
    "    if model.lower() == 'merton':\n",
    "        c1, c2, c4 = merton_cumulants(tau, r, theta)\n",
    "        a = log_ratio + c1 - L*np.sqrt(c2 + np.sqrt(c4))\n",
    "        b = log_ratio + c1 + L*np.sqrt(c2 + np.sqrt(c4))\n",
    "\n",
    "    elif model.lower() == 'kou':\n",
    "        c1, c2, c4 = kou_cumulants(tau, r, theta)\n",
    "        a = log_ratio + c1 - L*np.sqrt(c2 + np.sqrt(c4))\n",
    "        b = log_ratio + c1 + L*np.sqrt(c2 + np.sqrt(c4))\n",
    "\n",
    "    elif model.lower() == 'heston':\n",
    "        a = log_ratio - L*np.sqrt(tau)\n",
    "        b = log_ratio + L*np.sqrt(tau)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"unknown model\")\n",
    "\n",
    "    return a, b\n",
    "\n",
    "# ------------------  COS PAYOFF COEFFICIENTS  ------------------\n",
    "def chi_psi(k, a, b, c, d):\n",
    "    \"\"\"Compute the Xi(c,d) and Psi(c,d) vectors used in the COS method.\"\"\"\n",
    "    k = np.asarray(k, dtype=float)\n",
    "    omega = k * np.pi / (b - a)\n",
    "\n",
    "    # Allocate output arrays\n",
    "    chi = np.zeros_like(k, dtype=float)\n",
    "    psi = np.zeros_like(k, dtype=float)\n",
    "\n",
    "    # k = 0 terms \n",
    "    chi[0] = np.exp(d) - np.exp(c)\n",
    "    psi[0] = d - c\n",
    "\n",
    "    # k >= 1 terms\n",
    "    if k.size > 1:               \n",
    "        k_nz      = k[1:]\n",
    "        omega_nz  = omega[1:]\n",
    "        denom_nz  = 1.0 + omega_nz**2\n",
    "\n",
    "        chi[1:] = (\n",
    "            np.cos(omega_nz * (d - a)) * np.exp(d)\n",
    "          + omega_nz * np.sin(omega_nz * (d - a)) * np.exp(d)\n",
    "          - np.cos(omega_nz * (c - a)) * np.exp(c)\n",
    "          - omega_nz * np.sin(omega_nz * (c - a)) * np.exp(c)\n",
    "        ) / denom_nz\n",
    "\n",
    "        psi[1:] = (b - a) / (k_nz * np.pi) * (\n",
    "            np.sin(omega_nz * (d - a)) - np.sin(omega_nz * (c - a))\n",
    "        )\n",
    "    return chi, psi\n",
    "\n",
    "def payoff_coefficients(option_type, K, k, a, b):\n",
    "    \"\"\"Return the array H_k for a European call or put.\"\"\"\n",
    "    if option_type.lower() == 'call':\n",
    "        c, d = 0.0, b\n",
    "        chi, psi = chi_psi(k, a, b, c, d)\n",
    "        H_k = (2.0 / (b - a)) * K * (chi - psi)\n",
    "    else:                                       # put\n",
    "        c, d = a, 0.0\n",
    "        chi, psi = chi_psi(k, a, b, c, d)\n",
    "        H_k = (2.0 / (b - a)) * K * (-chi + psi)\n",
    "\n",
    "    return H_k      \n",
    "\n",
    "# ───────── COS PRICER ───────────────────────────────────────────\n",
    "def cos_price_single(model, option_type, S0, K, tau_sec, r, theta, N=256, L=8):\n",
    "    \"\"\"COS price for one European call or put.\"\"\"\n",
    "    tau = tau_sec / SEC_PER_YEAR\n",
    "    k   = np.arange(N, dtype=float)           # 0 … N‑1\n",
    "\n",
    "    # Model‑specific CF & [a,b]\n",
    "    if model == 'merton':\n",
    "        a, b = truncation_window('merton', S0, K, tau, r, theta, L=L)\n",
    "        phi_func  = phi_merton(S0, K, r, tau_sec, theta)\n",
    "\n",
    "    elif model == 'kou':\n",
    "        a, b = truncation_window('kou', S0, K, tau, r, theta, L=L)\n",
    "        phi_func = phi_kou(S0, K, r, tau_sec, theta)\n",
    "\n",
    "    elif model == 'heston':\n",
    "        a, b = truncation_window('heston', S0, K, tau, r, theta, L=L)\n",
    "        phi_func = phi_heston(S0, K, r, tau_sec, theta)\n",
    "    else:\n",
    "        raise ValueError(\"model must be 'merton', 'kou', or 'heston'\")\n",
    "\n",
    "    # --- Chi, Psi payoff coefficients --------------------------------\n",
    "    H_k = payoff_coefficients(option_type, K, k, a, b)\n",
    "\n",
    "    u       = k * np.pi / (b - a)\n",
    "    phi_u   = phi_func(u)                     # CF on grid\n",
    "    weights = np.real(phi_u * np.exp(-1j * u * a))\n",
    "    weights[0] *= 0.5                        # 0.5 weight for k=0\n",
    "\n",
    "    price = np.exp(-r * tau) * np.sum(weights * H_k)\n",
    "    return price\n",
    "\n",
    "def cos_price_batch(model, S0, K_vec, tau_sec_vec, r, option_type_vec, theta, N=256, L=8):\n",
    "    \"\"\"Price many options (mixed calls/puts) via a simple loop.\"\"\"\n",
    "    prices = np.empty_like(K_vec, dtype=float)\n",
    "    for j, (K, tau_s, opt) in enumerate(zip(K_vec, tau_sec_vec, option_type_vec)):\n",
    "        prices[j] = cos_price_single(model, opt, S0, K, tau_s, r, theta, N=N, L=L)\n",
    "    return prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a32d416",
   "metadata": {},
   "source": [
    "## Parameter Calibration Methods (`calibrate_merton`, `calibrate_kou`, `calibrate_heston`):\n",
    "These methods are responsible for calibrating the model parameters to match market data. The calibration process involves two steps:\n",
    "- **Global Search:** A global search is performed using `scipy.optimize.differential_evolution`, which explores the parameter space to find a promising region where the optimal parameters are likely to be.\n",
    "- **Local Refinement:** Once a promising region is found, local refinement is done using `scipy.optimize.minimize` with the BFGS algorithm, which fine-tunes the parameters to minimize the RMSE.\n",
    "\n",
    "These methods iteratively adjust the model parameters, ensuring that the model’s implied volatilities closely match the observed market-implied volatilities. Predefined bounds for each model parameter ensure that the optimizer searches within realistic and valid ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc637aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running calibrations…\n",
      "Calibrations finished.\n",
      "\n",
      "--- summary saved to calibration_results.csv ---\n",
      "  model        date    iv_rmse  n_strk moneyness_class  theta_1  theta_2  \\\n",
      "0   bsm  2021-09-07   4.489689      10             ATM  0.73826      NaN   \n",
      "1   bsm  2021-09-07  11.655000       2            DITM  1.01565      NaN   \n",
      "2   bsm  2021-09-07  11.655000       2            DOTM  1.01565      NaN   \n",
      "3   bsm  2021-09-07  16.734882      10             ITM  0.94670      NaN   \n",
      "4   bsm  2021-09-07  16.734882      10             OTM  0.94670      NaN   \n",
      "\n",
      "   theta_3  theta_4  theta_5  \n",
      "0      NaN      NaN      NaN  \n",
      "1      NaN      NaN      NaN  \n",
      "2      NaN      NaN      NaN  \n",
      "3      NaN      NaN      NaN  \n",
      "4      NaN      NaN      NaN  \n",
      "\n",
      "--- option-level metrics saved to options_with_model_metrics.csv ---\n"
     ]
    }
   ],
   "source": [
    "# Initial model configuration for calibration (initial guess, lower bound and upper bound)\n",
    "MODEL_CFG = {\n",
    "    'merton': (\n",
    "        np.array([0.40, 0.10, -0.10, 0.25]),\n",
    "        np.array([0.05, 0.00, -1.00, 0.05]),\n",
    "        np.array([3.00, 1.50,  1.00, 1.00])\n",
    "    ),\n",
    "    'kou': (\n",
    "        np.array([0.35, 1.00, 6.00, 0.20, 0.30]),  # σ, ξ, α1, α2, p1\n",
    "        np.array([0.05, 0.00, 1.10, 0.05, 0.05]),\n",
    "        np.array([2.00, 3.00, 10.00, 5.00, 0.95])\n",
    "    ),\n",
    "    'heston': (\n",
    "        np.array([1.00, 0.06, 0.40, -0.40, 0.06]), # κ, v̄, γ, ρ, v₀\n",
    "        np.array([0.30, 0.01, 0.05, -0.95, 0.01]),\n",
    "        np.array([5.00, 0.30, 1.00, 0.95, 0.50])\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def iv_newton(price, CP, S0, K, tau_sec, r, sigma_init=0.3, tol=1e-12, max_iter=30):\n",
    "    \"\"\"Vectorised Newton–Raphson implied‑vol solver.\"\"\"\n",
    "    sigma = np.full_like(price, sigma_init, dtype=float)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        diff  = bs_price(CP, S0, K, sigma, tau_sec, r) - price\n",
    "        vega  = bs_vega(S0, K, sigma, tau_sec, r)\n",
    "        sigma -= diff / np.where(vega > 1e-5, vega, np.inf)\n",
    "\n",
    "        if np.all(np.abs(diff) < tol):\n",
    "            break\n",
    "    sigma = np.where((sigma > 0) & (sigma < 5), sigma, np.nan)\n",
    "    return sigma\n",
    "\n",
    "def rmse_iv(model, S0, K, tau, CP, iv_mkt, theta, r=0.0):\n",
    "    \"\"\"RMSE between model IVs and market IVs for the specified model.\"\"\"\n",
    "    if model == 'heston':\n",
    "        kappa, vbar, gamma, rho, v0 = theta\n",
    "        if 2.0 * kappa * vbar <= gamma**2:     # variance may hit zero\n",
    "            return 1e5      \n",
    "        \n",
    "    price_mod = cos_price_batch(model, S0, K, tau, r, CP, theta)\n",
    "    iv_mod    = iv_newton(price_mod, CP, S0, K, tau, r, sigma_init=iv_mkt)\n",
    "\n",
    "    # Brent rescue on failed rows\n",
    "    bad = np.isnan(iv_mod)\n",
    "    for j in np.where(bad)[0]:\n",
    "        try:\n",
    "            iv_mod[j] = optimize.brentq(\n",
    "                lambda s: bs_price(CP[j], S0, K[j], s, tau[j], r) - price_mod[j],\n",
    "                1e-4, 5.0, maxiter=40)\n",
    "        except (ValueError, RuntimeError):\n",
    "            pass  # leave as NaN if still impossible\n",
    "\n",
    "    mask = np.isfinite(iv_mod) & np.isfinite(iv_mkt)\n",
    "    if not mask.any():               # all failed → return huge penalty\n",
    "        return 1e6\n",
    "\n",
    "    return 100*np.sqrt(np.mean((iv_mod[mask]-iv_mkt[mask])**2))\n",
    "\n",
    "def calibration_filter(df):\n",
    "    # Filter out ITM options\n",
    "    conditions =    ((df['opt_type'] == 'call') & (df['moneyness'] < 0.0)) | \\\n",
    "                    ((df['opt_type'] == 'put')  & (df['moneyness'] > 0.0))\n",
    "    \n",
    "    df = df.loc[~conditions].copy()\n",
    "\n",
    "    # Filter out one of the double ATM options with same strike at any day (otherwise it counts twice)\n",
    "    df = df.drop_duplicates(subset=['date','strike'], keep='first')\n",
    "\n",
    "    return df\n",
    "\n",
    "def calibrate_snapshot(df_snap, model, r=0.0):\n",
    "    \"\"\"Calibrate model parameters for one day's option surface.\"\"\"\n",
    "    if model not in MODEL_CFG:\n",
    "        raise ValueError(f\"unknown model: {model}\")\n",
    "\n",
    "    theta_0, lowerbound, upperbound = MODEL_CFG[model]\n",
    "    CP, S0_vec, K, tau, _, iv_mkt = extract_inputs_from_df(df_snap)\n",
    "    S0 = float(S0_vec[0])\n",
    "\n",
    "    loss = lambda theta: rmse_iv(model, S0, K, tau, CP, iv_mkt, theta, r=0.0)\n",
    "    res  = optimize.minimize(loss, theta_0, bounds=optimize.Bounds(lowerbound, upperbound), method='L-BFGS-B')\n",
    "\n",
    "    return {\n",
    "        'model':  model,\n",
    "        'theta':  res.x,\n",
    "        'iv_rmse': res.fun,\n",
    "        'n_strk':  len(df_snap),\n",
    "        'success': res.success,\n",
    "        'message': res.message\n",
    "    }\n",
    "\n",
    "def calibrate_bsm_snapshot(df_snap):\n",
    "    \"\"\"\n",
    "    Calibrates a single best-fit sigma for the BSM model against the daily IV surface.\n",
    "    \"\"\"\n",
    "    # We only need the market implied vols for this simple calibration\n",
    "    _, _, _, _, _, iv_mkt = extract_inputs_from_df(df_snap)\n",
    "\n",
    "    # The loss function is the RMSE between a single sigma and the market IVs\n",
    "    loss = lambda sigma: 100 * np.sqrt(np.mean((sigma - iv_mkt)**2))\n",
    "    \n",
    "    # Use mean of market IVs as a sensible starting guess\n",
    "    sigma_0 = np.mean(iv_mkt)\n",
    "\n",
    "    # Find the single best-fit sigma\n",
    "    res = optimize.minimize(loss, sigma_0, bounds=[(0.01, 5.0)], method='L-BFGS-B')\n",
    "\n",
    "    return {\n",
    "        'model':   'bsm',\n",
    "        'theta':   res.x,  # This will be the single best-fit sigma\n",
    "        'iv_rmse': res.fun,\n",
    "        'n_strk':  len(df_snap),\n",
    "        'success': res.success,\n",
    "        'message': res.message\n",
    "    }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df_btc, _ = load_0dte_data('08')\n",
    "    df_btc    = df_btc.dropna(subset=['mark_price', 'mark_iv'])\n",
    "    count_unfeasible = 0\n",
    "\n",
    "\n",
    "    models = {\n",
    "    'bsm'   : calibrate_bsm_snapshot,\n",
    "    'merton': lambda df: calibrate_snapshot(df, 'merton'),\n",
    "    'kou'   : lambda df: calibrate_snapshot(df, 'kou')\n",
    "}\n",
    "\n",
    "fits, option_frames = [], []\n",
    "skip = 0\n",
    "\n",
    "for date, df_day in df_btc.groupby(df_btc['timestamp'].dt.date):\n",
    "\n",
    "    df_use = calibration_filter(df_day)\n",
    "\n",
    "    if df_use['strike'].nunique() < 8:\n",
    "        print(f'Skip {date}: only {df_use.strike.nunique()} strikes')\n",
    "        skip += 1\n",
    "        continue\n",
    "\n",
    "    # -------- shared arrays for this snapshot --------------\n",
    "    CP, S0v, K, tau, _, iv_mkt = extract_inputs_from_df(df_use)\n",
    "    S0 = float(S0v[0])               # scalar\n",
    "    tau = tau.astype(float)          # ensure ndarray\n",
    "\n",
    "    for model, calib_func in models.items():\n",
    "\n",
    "        fit = calib_func(df_use)\n",
    "        fits.append({'date': date, **fit})\n",
    "\n",
    "        theta = fit['theta']\n",
    "\n",
    "        # ---- model prices & IV ----------------------------------\n",
    "        if model == 'bsm':\n",
    "            sigma_ = float(theta[0])\n",
    "            price_mod = bs_price(CP, S0, K, sigma_, tau, 0.0)\n",
    "            iv_mod    = np.full_like(iv_mkt, sigma_)\n",
    "        else:\n",
    "            price_mod = cos_price_batch(model, S0, K, tau, 0.0, CP, theta)\n",
    "            iv_mod    = iv_newton(price_mod, CP, S0, K, tau, 0.0,\n",
    "                                  sigma_init=iv_mkt)\n",
    "\n",
    "        enriched = df_use.copy()\n",
    "        enriched['model']        = model\n",
    "        enriched['price_model']  = price_mod\n",
    "        enriched['iv_model']     = iv_mod\n",
    "        enriched['iv_abs_err']   = np.abs(iv_mod - iv_mkt)\n",
    "\n",
    "        option_frames.append(enriched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b3694e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'opt_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/j1/sv4t8szd51jgm826w2bv1vyr0000gn/T/ipykernel_89273/1841992356.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m merton_df = calibration_results[calibration_results[\u001b[33m'model'\u001b[39m] == \u001b[33m'merton'\u001b[39m].copy()\n\u001b[32m     16\u001b[39m kou_df    = calibration_results[calibration_results[\u001b[33m'model'\u001b[39m] == \u001b[33m'kou'\u001b[39m].copy()\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Merge all on date + moneyness + opt_type\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m merged_df = bsm_df.merge(merton_df, on=[\u001b[33m'date'\u001b[39m, \u001b[33m'moneyness_class'\u001b[39m, \u001b[33m'opt_type'\u001b[39m], suffixes=(\u001b[33m'_bsm'\u001b[39m, \u001b[33m'_merton'\u001b[39m))\n\u001b[32m     20\u001b[39m merged_df = merged_df.merge(kou_df, on=[\u001b[33m'date'\u001b[39m, \u001b[33m'moneyness_class'\u001b[39m, \u001b[33m'opt_type'\u001b[39m])\n\u001b[32m     21\u001b[39m merged_df.rename(columns={\u001b[33m'iv_rmse'\u001b[39m: \u001b[33m'iv_rmse_kou'\u001b[39m}, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     22\u001b[39m \n",
      "\u001b[32m~/Documents/Master QF/Thesis/optimal-gamma-hedging/venv/lib/python3.13/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10835\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10836\u001b[39m     ) -> DataFrame:\n\u001b[32m  10837\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10838\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10839\u001b[39m         return merge(\n\u001b[32m  10840\u001b[39m             self,\n\u001b[32m  10841\u001b[39m             right,\n\u001b[32m  10842\u001b[39m             how=how,\n",
      "\u001b[32m~/Documents/Master QF/Thesis/optimal-gamma-hedging/venv/lib/python3.13/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32m~/Documents/Master QF/Thesis/optimal-gamma-hedging/venv/lib/python3.13/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32m~/Documents/Master QF/Thesis/optimal-gamma-hedging/venv/lib/python3.13/site-packages/pandas/core/reshape/merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1294\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1295\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1296\u001b[39m                         rk = cast(Hashable, rk)\n\u001b[32m   1297\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m                             right_keys.append(right._get_label_or_level_values(rk))\n\u001b[32m   1299\u001b[39m                         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1300\u001b[39m                             \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[32m   1301\u001b[39m                             right_keys.append(right.index._values)\n",
      "\u001b[32m~/Documents/Master QF/Thesis/optimal-gamma-hedging/venv/lib/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'opt_type'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and prepare\n",
    "calibration_results = pd.read_csv('/Users/joris/Documents/Master QF/Thesis/optimal-gamma-hedging/calibration_results.csv')\n",
    "calibration_results['date'] = pd.to_datetime(calibration_results['date'])\n",
    "\n",
    "# Keep only the desired moneyness classes\n",
    "keep_classes = ['ATM', 'OTM', 'DOTM']\n",
    "calibration_results = calibration_results[calibration_results['moneyness_class'].isin(keep_classes)]\n",
    "\n",
    "# Pivot data to get separate rows for each model\n",
    "bsm_df    = calibration_results[calibration_results['model'] == 'bsm'].copy()\n",
    "merton_df = calibration_results[calibration_results['model'] == 'merton'].copy()\n",
    "kou_df    = calibration_results[calibration_results['model'] == 'kou'].copy()\n",
    "\n",
    "# Merge all on date + moneyness + opt_type\n",
    "merged_df = bsm_df.merge(merton_df, on=['date', 'moneyness_class', 'opt_type'], suffixes=('_bsm', '_merton'))\n",
    "merged_df = merged_df.merge(kou_df, on=['date', 'moneyness_class', 'opt_type'])\n",
    "merged_df.rename(columns={'iv_rmse': 'iv_rmse_kou'}, inplace=True)\n",
    "\n",
    "# Compute relative RMSEs\n",
    "merged_df['rel_rmse_merton'] = merged_df['iv_rmse_merton'] / merged_df['iv_rmse_bsm']\n",
    "merged_df['rel_rmse_kou']    = merged_df['iv_rmse_kou']    / merged_df['iv_rmse_bsm']\n",
    "\n",
    "# Define order\n",
    "moneyness_classes = ['DOTM', 'OTM', 'ATM']\n",
    "option_types = ['call', 'put']\n",
    "\n",
    "# Plot\n",
    "for opt_type in option_types:\n",
    "    fig, ax = plt.subplots(len(moneyness_classes), 1, figsize=(10, 10), sharex=True)\n",
    "    for i, m_class in enumerate(moneyness_classes):\n",
    "        df_slice = merged_df[\n",
    "            (merged_df['moneyness_class'] == m_class) &\n",
    "            (merged_df['opt_type'] == opt_type)\n",
    "        ].sort_values('date')\n",
    "\n",
    "        if df_slice.empty:\n",
    "            continue\n",
    "\n",
    "        ax[i].plot(df_slice['date'], df_slice['rel_rmse_merton'], label='Merton/BSM', color='blue', marker='o')\n",
    "        ax[i].plot(df_slice['date'], df_slice['rel_rmse_kou'], label='Kou/BSM', color='orange', marker='o')\n",
    "        ax[i].set_title(f'{m_class} {opt_type.capitalize()}s')\n",
    "        ax[i].set_ylabel('Relative RMSE')\n",
    "        ax[i].grid(True)\n",
    "\n",
    "    ax[-1].set_xlabel('Date')\n",
    "    ax[0].legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Relative RMSE: {opt_type.capitalize()}s', fontsize=14, y=1.02)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2f5494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
